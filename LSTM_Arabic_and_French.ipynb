{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhENb2jXmIIZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41bb4d55-0fe6-45a4-ead9-53032eddaa67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.10/dist-packages (0.15.0)\n",
            "Collecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Installing collected packages: tensorflow-text\n",
            "Successfully installed tensorflow-text-2.15.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow tensorflow-hub tensorflow-text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing Libraries and Datasets**\n"
      ],
      "metadata": {
        "id": "UdJocJJNml4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the used libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential,load_model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense,Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from google.colab import drive\n"
      ],
      "metadata": {
        "id": "sjs5WatnmpCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/Files',force_remount=True) # Getting all files from my google drive account\n",
        "drive_path = '/content/Files/MyDrive/Deep Bonus Project/' # The path of the files on this colab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFCxlAEJmtoI",
        "outputId": "f4c99887-68b4-4f67-8b7e-a49bbbe32a13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/Files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Egyptian Sentiment Classification Model**"
      ],
      "metadata": {
        "id": "JuV38VRFnYbt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3zc0nm4twQCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "file_path = drive_path+'egyptian_dataset.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Shuffle the DataFrame\n",
        "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Splitting Sentences and labels\n",
        "sentences = data['sentence'].values\n",
        "labels = data['label'].values  # Labels are be 0 or 1\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_sentences, test_sentences, train_labels, test_labels = train_test_split(\n",
        "    sentences, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Tokenize the sentences\n",
        "# Out Of Vocab tokenizer: giving each word in the dataset an index, and if there is new word appeared in test it gets <OOV> token which is usually at index 0 or 1\n",
        "# for example on training dataset: oov: {<OOV>:1,is:2,cat:3,beutiful:4}\n",
        "# fot this sentence when converted to sequence: The cat is beutifull: [1,3,2,4]\n",
        "\n",
        "tokenizer = Tokenizer(oov_token='<OOV>') # Defining the tokenizer\n",
        "tokenizer.fit_on_texts(train_sentences)  # fitting the tokenizer on the training data\n",
        "\n",
        "# Convert sentences to sequences of numbers\n",
        "train_sequences = tokenizer.texts_to_sequences(train_sentences) # Using the tokenizer to covert from the sentences to sequences of indexes like: [1,3,2,4]\n",
        "test_sequences = tokenizer.texts_to_sequences(test_sentences) # Doing the same but on test data\n",
        "\n",
        "# Pad sequences to make them of the same length\n",
        "max_length = max(len(seq) for seq in train_sequences) # Getting the maximum length of a sentence\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post') # Applying padding of zeros (post method) to make all of the sequences of the same length\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post') # Doing the same on test dataset\n",
        "\n",
        "# Build the RNN model\n",
        "egyptian_model = Sequential() # Sequential model is a model of sequenced layers\n",
        "# Embedding layer\n",
        "# Input_Dim: size of vocabulary (unique words in the dataset)\n",
        "# Output_Dim: Embedding each word in a 16-dimensional vector\n",
        "# input_length: the length of the input sequences (which is the maximum length of all sentences)\n",
        "egyptian_model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=16, input_length=max_length))\n",
        "# Adding LSTM layer with 64 memory cell\n",
        "egyptian_model.add(LSTM(64))\n",
        "# Applying sigmoid to do classification\n",
        "egyptian_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "egyptian_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Huw7YKnxr5rF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting Sentences and labels\n",
        "sentences = data['sentence'].values\n",
        "labels = data['label'].values  # Labels are be 0 or 1\n"
      ],
      "metadata": {
        "id": "xeCHYXPKwRrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "\n",
        "egyptian_model_history = egyptian_model.fit(train_padded, train_labels, epochs=8, validation_data=(test_padded, test_labels), callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = egyptian_model.evaluate(test_padded, test_labels)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aNYrW6lneTz",
        "outputId": "5db961f2-bf3c-494b-bea6-43a78c1d7c4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "16/16 [==============================] - 4s 54ms/step - loss: 0.6936 - accuracy: 0.5040 - val_loss: 0.6927 - val_accuracy: 0.8031\n",
            "Epoch 2/8\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.6901 - accuracy: 0.6304 - val_loss: 0.6768 - val_accuracy: 0.8740\n",
            "Epoch 3/8\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.4489 - accuracy: 0.9308 - val_loss: 0.2285 - val_accuracy: 0.9134\n",
            "Epoch 4/8\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0833 - accuracy: 0.9802 - val_loss: 0.4666 - val_accuracy: 0.8819\n",
            "Epoch 5/8\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.0762 - accuracy: 0.9822 - val_loss: 0.3587 - val_accuracy: 0.9213\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.2285 - accuracy: 0.9134\n",
            "Test Accuracy: 91.34%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "egyptian_model.save(drive_path+'egyptian_model_8Epochs.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McUD6vhcqIHH",
        "outputId": "31bcdf00-972b-485a-eee3-5fe0ab3f0921"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "egyptian_model = load_model(drive_path+'egyptian_model_8Epochs.h5')"
      ],
      "metadata": {
        "id": "2DuOq27oqSQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(input_sentence):\n",
        "    # Tokenize and pad the input sentence\n",
        "    input_sequence = tokenizer.texts_to_sequences([input_sentence])\n",
        "    padded_input = pad_sequences(input_sequence, maxlen=max_length, padding='post')\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = egyptian_model.predict(padded_input)\n",
        "\n",
        "    # Convert the prediction to a sentiment label\n",
        "    sentiment_label = 'postivie' if predictions[0][0] > 0.5 else 'negative'\n",
        "\n",
        "    return sentiment_label\n"
      ],
      "metadata": {
        "id": "3_qhKS2RtfOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "input_sentence = input(\"Enter arabic sentence: \")\n",
        "predicted_sentiment = predict_sentiment(input_sentence)\n",
        "print(f\"Predicted Sentiment: {predicted_sentiment}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXCBY005tjW0",
        "outputId": "dc3871c5-fd47-48f0-dd9e-939a197f45db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter arabic sentence: البورصة المصرية في افضل احوالها\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Predicted Sentiment: postivie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fI_JfIOxuWCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **French Sentiment Classification Model**"
      ],
      "metadata": {
        "id": "JY5LDDUZuWZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "file_path = drive_path+'french_dataset.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Shuffle the DataFrame\n",
        "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Preprocess the data\n",
        "sentences = data['sentence'].values\n",
        "labels = data['label'].values  # Labels are be 0 or 1\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_sentences, test_sentences, train_labels, test_labels = train_test_split(\n",
        "    sentences, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "\n",
        "# Convert sentences to sequences of numbers\n",
        "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "\n",
        "# Pad sequences to make them of the same length\n",
        "max_length = max(len(seq) for seq in train_sequences)\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "# Build the RNN model\n",
        "french_model = Sequential()\n",
        "french_model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=16, input_length=max_length))\n",
        "french_model.add(LSTM(64))\n",
        "french_model.add(Dense(64, activation='relu'))\n",
        "french_model.add(Dropout(0.5))\n",
        "french_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "french_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "YVwZ5N5SuWZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "french_model_history = french_model.fit(train_padded, train_labels, epochs=8, validation_data=(test_padded, test_labels), callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = french_model.evaluate(test_padded, test_labels)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da37a2b3-0c08-4983-e3f7-f6b8d00606ef",
        "id": "ftHc2E-yuWZ2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "250/250 [==============================] - 9s 28ms/step - loss: 0.6934 - accuracy: 0.5006 - val_loss: 0.6931 - val_accuracy: 0.4950\n",
            "Epoch 2/8\n",
            "250/250 [==============================] - 8s 32ms/step - loss: 0.6949 - accuracy: 0.5041 - val_loss: 0.6924 - val_accuracy: 0.5100\n",
            "Epoch 3/8\n",
            "250/250 [==============================] - 6s 25ms/step - loss: 0.6056 - accuracy: 0.6862 - val_loss: 0.5737 - val_accuracy: 0.7075\n",
            "Epoch 4/8\n",
            "250/250 [==============================] - 8s 34ms/step - loss: 0.4107 - accuracy: 0.8288 - val_loss: 0.5741 - val_accuracy: 0.7075\n",
            "Epoch 5/8\n",
            "250/250 [==============================] - 6s 25ms/step - loss: 0.2891 - accuracy: 0.8978 - val_loss: 0.6941 - val_accuracy: 0.7060\n",
            "Epoch 6/8\n",
            "250/250 [==============================] - 9s 35ms/step - loss: 0.2164 - accuracy: 0.9280 - val_loss: 0.8530 - val_accuracy: 0.6825\n",
            "63/63 [==============================] - 1s 8ms/step - loss: 0.5737 - accuracy: 0.7075\n",
            "Test Accuracy: 70.75%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "french_model.save(drive_path+'french_model_8Epochs.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22a7e290-f368-4d2a-e0ad-f829f5af84dc",
        "id": "RAk88roFuWZ3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "french_model = load_model(drive_path+'french_model_8Epochs.h5')"
      ],
      "metadata": {
        "id": "aUC_XN7EuWZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(input_sentence):\n",
        "    # Tokenize and pad the input sentence\n",
        "    input_sequence = tokenizer.texts_to_sequences([input_sentence])\n",
        "    padded_input = pad_sequences(input_sequence, maxlen=max_length, padding='post')\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = french_model.predict(padded_input)\n",
        "\n",
        "    # Convert the prediction to a sentiment label\n",
        "    sentiment_label = 'postivie' if predictions[0][0] > 0.5 else 'negative'\n",
        "\n",
        "    return sentiment_label\n"
      ],
      "metadata": {
        "id": "0nmvCf__uWZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "input_sentence = input(\"Enter french sentence: \")\n",
        "predicted_sentiment = predict_sentiment(input_sentence)\n",
        "print(f\"Predicted Sentiment: {predicted_sentiment}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e815df7f-8a12-433b-fe66-3c1eba3a719f",
        "id": "F425dd2auWZ6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter french sentence: C'est comme si mon cœur se brisait de tristesse\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Predicted Sentiment: negative\n"
          ]
        }
      ]
    }
  ]
}
